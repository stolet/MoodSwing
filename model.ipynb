{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "# initial data loading cell\n",
    "\n",
    "# !wget http://emodb.bilderbar.info/download/download.zip\n",
    "# !mkdir -p data/berlin && unzip download.zip -d data/berlin\n",
    "\n",
    "# # !aria2c -x 16 https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\n",
    "# !wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\n",
    "# !mkdir -p data/ravdess && unzip Audio_Speech_Actors_01-24.zip -d data/ravdess\n",
    "# !rm download.zip Audio_Speech_Actors_01-24.zip\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "RAVDESS = \"data/ravdess/\"\n",
    "BERLIN = \"data/berlin/wav/\"\n",
    "RAVDESS_CLEAN = \"clean_data/ravdess_clean.npy\"\n",
    "BERLIN_CLEAN = \"clean_data/berlin_clean.npy\"\n",
    "# !rm {RAVDESS_CLEAN}\n",
    "# !rm {BERLIN_CLEAN}\n",
    "DEV_MODE = False\n",
    "DEV_LIMIT = 20\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# AUDIO = 0\n",
    "# SAMPLING_RATE = 44100//2\n",
    "# EMOTION = 2\n",
    "# ACTOR = 3\n",
    "# GENDER = 4\n",
    "MFCC = 20\n",
    "\n",
    "# Possible Emotions\n",
    "#  - Neutral\n",
    "#  - Calm\n",
    "#  - Happy\n",
    "#  - Sad\n",
    "#  - Angry\n",
    "#  - Fearful\n",
    "#  - Disgust\n",
    "#  - Surprised\n",
    "#  - Boredom\n",
    "\n",
    "# Emotion = {'01': neutral, '02': calm, '03': happy, '04': sad, '05': angry, '06': fearful, '07': disgust, '08': surprised}\n",
    "\n",
    "# Possible genders\n",
    "#  - male\n",
    "#  - female\n",
    "\n",
    "# 34 possible actors\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.berlin_data = None\n",
    "        self.ravdess_data = None\n",
    "        self.noisy_berlin_data = None\n",
    "        self.noisey_ravdess_data = None\n",
    "        self.feature_names = [\"audio\", \"sampling_rate\", \"emotion\", \"actor\", \"gender\"]\n",
    "        self._normalized = False\n",
    "        self._augmented = False\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.ravdess_data = np.load(RAVDESS_CLEAN, allow_pickle=True)\n",
    "            self.berlin_data = np.load(BERLIN_CLEAN, allow_pickle=True)\n",
    "        except:  # no file found, so do the normal loading stuff\n",
    "            print(\"Manually rebuilding the dataset!\")\n",
    "            self.ravdess_data = self.fetch(RAVDESS, RAVDESS_CLEAN)\n",
    "            self.berlin_data = self.fetch(BERLIN, BERLIN_CLEAN)\n",
    "\n",
    "    def fetch(self, path, save_path):\n",
    "        #         n = Dataset._get_num_files(path)\n",
    "        output = []  # np.zeros(n, dtype=object)\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if DEV_MODE and len(output) > DEV_LIMIT:\n",
    "                    continue\n",
    "                if \".wav\" in file:\n",
    "                    element = {\n",
    "                        \"audio\": None,\n",
    "                        \"sampling_rate\": None,\n",
    "                        \"emotion\": None,\n",
    "                        \"actor\": None,\n",
    "                        \"gender\": None,\n",
    "                        \"mfcc\": None,\n",
    "                    }\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    audio, sampling_rate = librosa.load(filepath)\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=MFCC)\n",
    "                    emotion = Dataset._get_emotion(file, path)\n",
    "                    actor = Dataset._get_actor(file, path)\n",
    "                    gender = Dataset._get_gender(file, path)\n",
    "\n",
    "                    element[\"audio\"] = audio\n",
    "                    element[\"sampling_rate\"] = sampling_rate\n",
    "                    element[\"emotion\"] = emotion\n",
    "                    element[\"actor\"] = actor\n",
    "                    element[\"gender\"] = gender\n",
    "                    element[\"mfcc\"] = mfcc\n",
    "                    output.append(element)\n",
    "            #                 output[i] = element\n",
    "\n",
    "        output = np.array(output, dtype=object)\n",
    "        np.save(save_path, output)\n",
    "        return output\n",
    "\n",
    "    def normalize(self):\n",
    "        if self._normalized == False:\n",
    "            for elt in self.ravdess_data:\n",
    "                mean = np.mean(elt[\"mfcc\"], axis=0)\n",
    "                std = np.std(elt[\"mfcc\"], axis=0)\n",
    "                elt[\"mfcc\"] = (elt[\"mfcc\"] - mean) / std\n",
    "        self._normalized = True\n",
    "\n",
    "    def augment_data(self, noisefactor=2, shiftmax=0.5, shiftdir=\"both\"):\n",
    "        noisy_data = self.get_noisy_data(noisefactor)\n",
    "        shifted_data = self.get_shifted_data(shiftmax, shiftdir)\n",
    "        self.ravdess_data = np.append(self.ravdess_data, noisy_data)\n",
    "        self.ravdess_data = np.append(self.ravdess_data, shifted_data)\n",
    "\n",
    "    def get_noisy_data(self, noise_factor=1):\n",
    "        noisy_data = np.array([])\n",
    "        for elt in self.ravdess_data:\n",
    "            newelt = deepcopy(elt)\n",
    "            noise = np.random.randn(newelt[\"mfcc\"].shape[0], newelt[\"mfcc\"].shape[1])\n",
    "            newelt[\"mfcc\"] = newelt[\"mfcc\"] + noise * noise_factor\n",
    "            noisy_data = np.append(noisy_data, newelt)\n",
    "        return noisy_data\n",
    "\n",
    "    def get_shifted_data(self, shift_max, shift_direction):\n",
    "        shifted_data = np.array([])\n",
    "        for elt in self.ravdess_data:\n",
    "            newelt = deepcopy(elt)\n",
    "            shift = np.random.randint(newelt[\"sampling_rate\"] * shift_max)\n",
    "            if shift_direction == \"left\":\n",
    "                shift = -shift\n",
    "            elif shift_direction == \"both\":\n",
    "                direction = np.random.randint(0, 2)\n",
    "                if direction == 1:\n",
    "                    shift = -shift\n",
    "\n",
    "            shifted_audio = np.roll(elt[\"audio\"], shift)\n",
    "            if shift > 0:\n",
    "                shifted_audio[:shift] = 0\n",
    "            else:\n",
    "                shifted_audio[shift:] = 0\n",
    "            newelt[\"audio\"] = shifted_audio\n",
    "            newelt[\"mfcc\"] = librosa.feature.mfcc(\n",
    "                y=newelt[\"audio\"], sr=newelt[\"sampling_rate\"], n_mfcc=MFCC\n",
    "            )\n",
    "            shifted_data = np.append(shifted_data, newelt)\n",
    "        return shifted_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_num_files(path):\n",
    "        total = 0\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            total += len(files)\n",
    "        return total\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_emotion(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            return re.findall(\"[0-9][0-9]\", filename)[2]\n",
    "        elif dataset_type == BERLIN:\n",
    "            emotion = re.search(\"[A-Z]\", filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_actor(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            return re.findall(\"[0-9][0-9]\", filename)[6]\n",
    "        elif dataset_type == BERLIN:\n",
    "            actor = re.search(\"[a-z][0-9][0-9]\", filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_gender(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            actor = re.findall(\"[0-9][0-9]\", filename)[6]\n",
    "            if int(actor) % 2 == 0:\n",
    "                return \"female\"\n",
    "            else:\n",
    "                return \"male\"\n",
    "        elif dataset_type == BERLIN:\n",
    "            actor = re.findall(\"[0-9][0-9]\", filename)[0]\n",
    "            return Dataset._get_berlin_gender_from_actor(actor)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_berlin_gender_from_actor(actor):\n",
    "        switch = {\n",
    "            \"03\": \"male\",\n",
    "            \"08\": \"female\",\n",
    "            \"09\": \"female\",\n",
    "            \"10\": \"male\",\n",
    "            \"11\": \"male\",\n",
    "            \"12\": \"male\",\n",
    "            \"13\": \"female\",\n",
    "            \"14\": \"female\",\n",
    "            \"15\": \"male\",\n",
    "            \"16\": \"female\",\n",
    "        }\n",
    "        return switch.get(actor, \"Invalid actor!\")\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.augment_data()\n",
    "dataset.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from typing import Dict, Tuple, List\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def pad(to_pad, r=MFCC, c=230):\n",
    "    z = np.zeros((r, c))\n",
    "    z[: to_pad.shape[0], : to_pad.shape[1]] = to_pad\n",
    "    return z\n",
    "\n",
    "\n",
    "# X = (lambda x: pad(x['mfcc']))(dataset.ravdess_data)\n",
    "def prep_Xy(data):\n",
    "    #     padded = pad_sequence([torch.from_numpy(x['mfcc']) for x in dataset.ravdess_data]) #https://pytorch.org/docs/stable/nn.html#pad-sequence\n",
    "    X = torch.tensor([pad(x[\"mfcc\"]) for x in data.ravdess_data], dtype=torch.float32)\n",
    "    y = torch.tensor([int(x[\"emotion\"]) - 1 for x in data.ravdess_data])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class PaddedDataset(Dataset):\n",
    "    def __init__(self, X: torch.tensor, y: torch.tensor):\n",
    "        super(PaddedDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].unsqueeze(0).to(device), self.y[index].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "\n",
    "paddedDataset = PaddedDataset(*prep_Xy(dataset))\n",
    "# split 70% for training, 15% for validation, 15% for test\n",
    "train_size = int(0.7 * len(paddedDataset))\n",
    "valid_size = int(0.15 * len(paddedDataset))\n",
    "test_size = len(paddedDataset) - (train_size + valid_size)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    paddedDataset, [train_size, valid_size, test_size]\n",
    ")\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "\n",
    "# from collections import Counter\n",
    "# Counter(prep_Xy(dataset)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 2.054 | Train Acc: 16.14%\n",
      "\t Val. Loss: 1.990 |  Val. Acc: 18.87%\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.938 | Train Acc: 24.08%\n",
      "\t Val. Loss: 1.836 |  Val. Acc: 26.62%\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.752 | Train Acc: 30.86%\n",
      "\t Val. Loss: 1.682 |  Val. Acc: 33.18%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.669 | Train Acc: 34.76%\n",
      "\t Val. Loss: 1.611 |  Val. Acc: 35.69%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.627 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.582 |  Val. Acc: 36.46%\n",
      "Epoch: 06 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 1.593 | Train Acc: 37.02%\n",
      "\t Val. Loss: 1.558 |  Val. Acc: 38.72%\n",
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.559 | Train Acc: 38.21%\n",
      "\t Val. Loss: 1.544 |  Val. Acc: 39.79%\n",
      "Epoch: 08 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.528 | Train Acc: 40.26%\n",
      "\t Val. Loss: 1.523 |  Val. Acc: 40.10%\n",
      "Epoch: 09 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.492 | Train Acc: 41.84%\n",
      "\t Val. Loss: 1.497 |  Val. Acc: 40.87%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 1.444 | Train Acc: 43.89%\n",
      "\t Val. Loss: 1.466 |  Val. Acc: 41.49%\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.384 | Train Acc: 45.70%\n",
      "\t Val. Loss: 1.429 |  Val. Acc: 44.56%\n",
      "Epoch: 12 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.318 | Train Acc: 48.02%\n",
      "\t Val. Loss: 1.411 |  Val. Acc: 45.74%\n",
      "Epoch: 13 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.246 | Train Acc: 51.45%\n",
      "\t Val. Loss: 1.397 |  Val. Acc: 48.67%\n",
      "Epoch: 14 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.170 | Train Acc: 55.22%\n",
      "\t Val. Loss: 1.333 |  Val. Acc: 50.21%\n",
      "Epoch: 15 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.087 | Train Acc: 58.60%\n",
      "\t Val. Loss: 1.278 |  Val. Acc: 54.82%\n",
      "Epoch: 16 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.005 | Train Acc: 61.67%\n",
      "\t Val. Loss: 1.249 |  Val. Acc: 55.44%\n",
      "Epoch: 17 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.922 | Train Acc: 65.17%\n",
      "\t Val. Loss: 1.199 |  Val. Acc: 60.51%\n",
      "Epoch: 18 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.854 | Train Acc: 68.36%\n",
      "\t Val. Loss: 1.202 |  Val. Acc: 59.90%\n",
      "Epoch: 19 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.782 | Train Acc: 70.83%\n",
      "\t Val. Loss: 1.206 |  Val. Acc: 59.59%\n",
      "Epoch: 20 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.708 | Train Acc: 73.71%\n",
      "\t Val. Loss: 1.169 |  Val. Acc: 64.67%\n",
      "Epoch: 21 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.598 | Train Acc: 79.36%\n",
      "\t Val. Loss: 1.276 |  Val. Acc: 62.67%\n",
      "Epoch: 22 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.553 | Train Acc: 79.66%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 65.90%\n",
      "Epoch: 23 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.467 | Train Acc: 83.40%\n",
      "\t Val. Loss: 1.299 |  Val. Acc: 63.28%\n",
      "Epoch: 24 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.437 | Train Acc: 83.63%\n",
      "\t Val. Loss: 1.380 |  Val. Acc: 65.13%\n",
      "Epoch: 25 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.401 | Train Acc: 85.88%\n",
      "\t Val. Loss: 1.326 |  Val. Acc: 67.28%\n",
      "Epoch: 26 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.329 | Train Acc: 89.16%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 71.08%\n",
      "Epoch: 27 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.302 | Train Acc: 89.75%\n",
      "\t Val. Loss: 1.315 |  Val. Acc: 69.23%\n",
      "Epoch: 28 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.234 | Train Acc: 91.44%\n",
      "\t Val. Loss: 1.366 |  Val. Acc: 71.38%\n",
      "Epoch: 29 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.234 | Train Acc: 91.74%\n",
      "\t Val. Loss: 1.296 |  Val. Acc: 72.31%\n",
      "Epoch: 30 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.213 | Train Acc: 92.53%\n",
      "\t Val. Loss: 1.444 |  Val. Acc: 70.62%\n",
      "Epoch: 31 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.235 | Train Acc: 91.54%\n",
      "\t Val. Loss: 1.267 |  Val. Acc: 73.08%\n",
      "Epoch: 32 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.195 | Train Acc: 93.36%\n",
      "\t Val. Loss: 1.336 |  Val. Acc: 73.69%\n",
      "Epoch: 33 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.168 | Train Acc: 94.55%\n",
      "\t Val. Loss: 1.471 |  Val. Acc: 70.46%\n",
      "Epoch: 34 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.118 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.502 |  Val. Acc: 73.23%\n",
      "Epoch: 35 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.113 | Train Acc: 95.83%\n",
      "\t Val. Loss: 1.392 |  Val. Acc: 76.00%\n",
      "Epoch: 36 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.79%\n",
      "\t Val. Loss: 1.671 |  Val. Acc: 71.28%\n",
      "Epoch: 37 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.081 | Train Acc: 97.55%\n",
      "\t Val. Loss: 1.496 |  Val. Acc: 76.46%\n",
      "Epoch: 38 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.116 | Train Acc: 96.23%\n",
      "\t Val. Loss: 1.585 |  Val. Acc: 75.69%\n",
      "Epoch: 39 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.040 | Train Acc: 99.31%\n",
      "\t Val. Loss: 2.113 |  Val. Acc: 71.54%\n",
      "Epoch: 40 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.41%\n",
      "\t Val. Loss: 1.568 |  Val. Acc: 76.15%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 2 * 54, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=8)\n",
    "\n",
    "    # comes in as 20x230. 20 = MFCCs, 230 is padded length that doesnt truncate any data\n",
    "    def forward(self, t):\n",
    "        # conv 1\n",
    "        t = self.conv1(t)  # output 6x16x226\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)  # output 6x8x113\n",
    "\n",
    "        # conv 2\n",
    "        t = self.conv2(t)  # output 12x4x109\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)  # output 12x2x54\n",
    "\n",
    "        # fc1\n",
    "        t = t.reshape(-1, 12 * 2 * 54)  # flatten to 1x1296\n",
    "        t = self.fc1(t)  # output 1x120\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # fc2\n",
    "        t = self.fc2(t)  # output 1x60\n",
    "        t = F.relu(t)\n",
    "        t = self.out(t)  # output 1x8 (8 classes)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "        return t\n",
    "\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    )  # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for X, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = categorical_accuracy(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in iterator:\n",
    "            predictions = model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            acc = categorical_accuracy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "# def train(model, train_loader, optimizer, epochs=10, log_interval=100, device=device):\n",
    "#     model.to(device).train()\n",
    "#     for batch_idx, (X, y) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(X)\n",
    "#         loss = F.cross_entropy(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print(\n",
    "#                 \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "#                     epochs,\n",
    "#                     batch_idx * len(X),\n",
    "#                     len(train_loader.dataset),\n",
    "#                     100.0 * batch_idx / len(train_loader),\n",
    "#                     loss.item(),\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "\n",
    "# def test(model, test_loader, device=device):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in test_loader:\n",
    "#             output = model(X)\n",
    "#             test_loss += F.cross_entropy(output, y).item()  # sum up batch loss\n",
    "#             pred = output.argmax(\n",
    "#                 dim=1, keepdim=True\n",
    "#             )  # get the index of the max log-probability\n",
    "#             correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print(\n",
    "#         \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "#             test_loss,\n",
    "#             correct,\n",
    "#             len(test_loader.dataset),\n",
    "#             100.0 * correct / len(test_loader.dataset),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "model = Network().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# train(net, trainloader, optimizer)\n",
    "# test(net, testloader)\n",
    "N_EPOCHS = 40\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"classifer-model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
