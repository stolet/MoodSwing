{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "--2019-11-28 22:04:55--  http://emodb.bilderbar.info/download/download.zip\r\n",
      "Resolving emodb.bilderbar.info (emodb.bilderbar.info)... 85.13.147.80\r\n",
      "Connecting to emodb.bilderbar.info (emodb.bilderbar.info)|85.13.147.80|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 40567066 (39M) [application/zip]\r\n",
      "Saving to: ‘download.zip.1’\r\n",
      "\r\n",
      "download.zip.1      100%[===================>]  38.69M   960KB/s    in 44s     \r\n",
      "\r\n",
      "2019-11-28 22:05:40 (911 KB/s) - ‘download.zip.1’ saved [40567066/40567066]\r\n",
      "\r\n",
      "Archive:  download.zip\r\n",
      "caution: filename not matched:  -o\r\n",
      "--2019-11-28 22:05:42--  https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\r\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\r\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.95.95\r\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.95.95|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 208468073 (199M) [application/octet-stream]\r\n",
      "Saving to: ‘Audio_Speech_Actors_01-24.zip.1’\r\n",
      "\r\n",
      "Audio_Speech_Actors 100%[===================>] 198.81M  1.16MB/s    in 4m 27s  \r\n",
      "\r\n",
      "2019-11-28 22:10:12 (761 KB/s) - ‘Audio_Speech_Actors_01-24.zip.1’ saved [208468073/208468073]\r\n",
      "\r\n",
      "Archive:  Audio_Speech_Actors_01-24.zip\r\n",
      "caution: filename not matched:  -o\r\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-812f150c59ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p data/ravdess && unzip Audio_Speech_Actors_01-24.zip -d data/ravdess -o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm download.zip Audio_Speech_Actors_01-24.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lab_black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</usr/lib/python3.8/site-packages/decorator.py:decorator-gen-64>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_black'"
     ],
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_black'",
     "output_type": "error"
    }
   ],
   "source": [
    "# initial data loading cell\n",
    "\n",
    "!wget http://emodb.bilderbar.info/download/download.zip\n",
    "!mkdir -p data/berlin && unzip download.zip -d data/berlin\n",
    "\n",
    "# !aria2c -x 16 https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\n",
    "!wget https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip\n",
    "!mkdir -p data/ravdess && unzip Audio_Speech_Actors_01-24.zip -d data/ravdess\n",
    "!rm download.zip Audio_Speech_Actors_01-24.zip\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "RAVDESS = \"data/ravdess/\"\n",
    "BERLIN = \"data/berlin/wav/\"\n",
    "RAVDESS_CLEAN = \"clean_data/ravdess_clean.npy\"\n",
    "BERLIN_CLEAN = \"clean_data/berlin_clean.npy\"\n",
    "# !rm {RAVDESS_CLEAN}\n",
    "# !rm {BERLIN_CLEAN}\n",
    "DEV_MODE = False\n",
    "DEV_LIMIT = 20\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# AUDIO = 0\n",
    "# SAMPLING_RATE = 44100//2\n",
    "# EMOTION = 2\n",
    "# ACTOR = 3\n",
    "# GENDER = 4\n",
    "MFCC = 20\n",
    "\n",
    "# Possible Emotions\n",
    "#  - Neutral\n",
    "#  - Calm\n",
    "#  - Happy\n",
    "#  - Sad\n",
    "#  - Angry\n",
    "#  - Fearful\n",
    "#  - Disgust\n",
    "#  - Surprised\n",
    "#  - Boredom\n",
    "\n",
    "# Emotion = {'01': neutral, '02': calm, '03': happy, '04': sad, '05': angry, '06': fearful, '07': disgust, '08': surprised}\n",
    "\n",
    "# Possible genders\n",
    "#  - male\n",
    "#  - female\n",
    "\n",
    "# 34 possible actors\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.berlin_data = None\n",
    "        self.ravdess_data = None\n",
    "        self.noisy_berlin_data = None\n",
    "        self.noisey_ravdess_data = None\n",
    "        self.feature_names = [\"audio\", \"sampling_rate\", \"emotion\", \"actor\", \"gender\"]\n",
    "        self._normalized = False\n",
    "        self._augmented = False\n",
    "\n",
    "    def load(self):\n",
    "        try:\n",
    "            self.ravdess_data = np.load(RAVDESS_CLEAN, allow_pickle=True)\n",
    "            self.berlin_data = np.load(BERLIN_CLEAN, allow_pickle=True)\n",
    "        except:  # no file found, so do the normal loading stuff\n",
    "            print(\"Manually rebuilding the dataset!\")\n",
    "            self.ravdess_data = self.fetch(RAVDESS, RAVDESS_CLEAN)\n",
    "            self.berlin_data = self.fetch(BERLIN, BERLIN_CLEAN)\n",
    "\n",
    "    def fetch(self, path, save_path):\n",
    "        #         n = Dataset._get_num_files(path)\n",
    "        output = []  # np.zeros(n, dtype=object)\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if DEV_MODE and len(output) > DEV_LIMIT:\n",
    "                    continue\n",
    "                if \".wav\" in file:\n",
    "                    element = {\n",
    "                        \"audio\": None,\n",
    "                        \"sampling_rate\": None,\n",
    "                        \"emotion\": None,\n",
    "                        \"actor\": None,\n",
    "                        \"gender\": None,\n",
    "                        \"mfcc\": None,\n",
    "                    }\n",
    "                    filepath = os.path.join(root, file)\n",
    "                    audio, sampling_rate = librosa.load(filepath)\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=MFCC)\n",
    "                    emotion = Dataset._get_emotion(file, path)\n",
    "                    actor = Dataset._get_actor(file, path)\n",
    "                    gender = Dataset._get_gender(file, path)\n",
    "\n",
    "                    element[\"audio\"] = audio\n",
    "                    element[\"sampling_rate\"] = sampling_rate\n",
    "                    element[\"emotion\"] = emotion\n",
    "                    element[\"actor\"] = actor\n",
    "                    element[\"gender\"] = gender\n",
    "                    element[\"mfcc\"] = mfcc\n",
    "                    output.append(element)\n",
    "            #                 output[i] = element\n",
    "\n",
    "        output = np.array(output, dtype=object)\n",
    "        np.save(save_path, output)\n",
    "        return output\n",
    "\n",
    "    def normalize(self):\n",
    "        if self._normalized == False:\n",
    "            for elt in self.ravdess_data:\n",
    "                mean = np.mean(elt[\"mfcc\"], axis=0)\n",
    "                std = np.std(elt[\"mfcc\"], axis=0)\n",
    "                elt[\"mfcc\"] = (elt[\"mfcc\"] - mean) / std\n",
    "        self._normalized = True\n",
    "\n",
    "    def augment_data(self, noisefactor=2, shiftmax=0.5, shiftdir=\"both\"):\n",
    "        noisy_data = self.get_noisy_data(noisefactor)\n",
    "        shifted_data = self.get_shifted_data(shiftmax, shiftdir)\n",
    "        self.ravdess_data = np.append(self.ravdess_data, noisy_data)\n",
    "        self.ravdess_data = np.append(self.ravdess_data, shifted_data)\n",
    "\n",
    "    def get_noisy_data(self, noise_factor=1):\n",
    "        noisy_data = np.array([])\n",
    "        for elt in self.ravdess_data:\n",
    "            newelt = deepcopy(elt)\n",
    "            noise = np.random.randn(newelt[\"mfcc\"].shape[0], newelt[\"mfcc\"].shape[1])\n",
    "            newelt[\"mfcc\"] = newelt[\"mfcc\"] + noise * noise_factor\n",
    "            noisy_data = np.append(noisy_data, newelt)\n",
    "        return noisy_data\n",
    "\n",
    "    def get_shifted_data(self, shift_max, shift_direction):\n",
    "        shifted_data = np.array([])\n",
    "        for elt in self.ravdess_data:\n",
    "            newelt = deepcopy(elt)\n",
    "            shift = np.random.randint(newelt[\"sampling_rate\"] * shift_max)\n",
    "            if shift_direction == \"left\":\n",
    "                shift = -shift\n",
    "            elif shift_direction == \"both\":\n",
    "                direction = np.random.randint(0, 2)\n",
    "                if direction == 1:\n",
    "                    shift = -shift\n",
    "\n",
    "            shifted_audio = np.roll(elt[\"audio\"], shift)\n",
    "            if shift > 0:\n",
    "                shifted_audio[:shift] = 0\n",
    "            else:\n",
    "                shifted_audio[shift:] = 0\n",
    "            newelt[\"audio\"] = shifted_audio\n",
    "            newelt[\"mfcc\"] = librosa.feature.mfcc(\n",
    "                y=newelt[\"audio\"], sr=newelt[\"sampling_rate\"], n_mfcc=MFCC\n",
    "            )\n",
    "            shifted_data = np.append(shifted_data, newelt)\n",
    "        return shifted_data\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_num_files(path):\n",
    "        total = 0\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            total += len(files)\n",
    "        return total\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_emotion(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            return re.findall(\"[0-9][0-9]\", filename)[2]\n",
    "        elif dataset_type == BERLIN:\n",
    "            emotion = re.search(\"[A-Z]\", filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_actor(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            return re.findall(\"[0-9][0-9]\", filename)[6]\n",
    "        elif dataset_type == BERLIN:\n",
    "            actor = re.search(\"[a-z][0-9][0-9]\", filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_gender(filename, dataset_type):\n",
    "        if dataset_type == RAVDESS:\n",
    "            actor = re.findall(\"[0-9][0-9]\", filename)[6]\n",
    "            if int(actor) % 2 == 0:\n",
    "                return \"female\"\n",
    "            else:\n",
    "                return \"male\"\n",
    "        elif dataset_type == BERLIN:\n",
    "            actor = re.findall(\"[0-9][0-9]\", filename)[0]\n",
    "            return Dataset._get_berlin_gender_from_actor(actor)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_berlin_gender_from_actor(actor):\n",
    "        switch = {\n",
    "            \"03\": \"male\",\n",
    "            \"08\": \"female\",\n",
    "            \"09\": \"female\",\n",
    "            \"10\": \"male\",\n",
    "            \"11\": \"male\",\n",
    "            \"12\": \"male\",\n",
    "            \"13\": \"female\",\n",
    "            \"14\": \"female\",\n",
    "            \"15\": \"male\",\n",
    "            \"16\": \"female\",\n",
    "        }\n",
    "        return switch.get(actor, \"Invalid actor!\")\n",
    "\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.augment_data()\n",
    "dataset.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from typing import Dict, Tuple, List\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def pad(to_pad, r=MFCC, c=230):\n",
    "    z = np.zeros((r, c))\n",
    "    z[: to_pad.shape[0], : to_pad.shape[1]] = to_pad\n",
    "    return z\n",
    "\n",
    "\n",
    "# X = (lambda x: pad(x['mfcc']))(dataset.ravdess_data)\n",
    "def prep_Xy(data):\n",
    "    #     padded = pad_sequence([torch.from_numpy(x['mfcc']) for x in dataset.ravdess_data]) #https://pytorch.org/docs/stable/nn.html#pad-sequence\n",
    "    X = torch.tensor([pad(x[\"mfcc\"]) for x in data.ravdess_data], dtype=torch.float32)\n",
    "    y = torch.tensor([int(x[\"emotion\"]) - 1 for x in data.ravdess_data])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class PaddedDataset(Dataset):\n",
    "    def __init__(self, X: torch.tensor, y: torch.tensor):\n",
    "        super(PaddedDataset).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index].unsqueeze(0).to(device), self.y[index].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "\n",
    "paddedDataset = PaddedDataset(*prep_Xy(dataset))\n",
    "# split 70% for training, 15% for validation, 15% for test\n",
    "train_size = int(0.7 * len(paddedDataset))\n",
    "valid_size = int(0.15 * len(paddedDataset))\n",
    "test_size = len(paddedDataset) - (train_size + valid_size)\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    paddedDataset, [train_size, valid_size, test_size]\n",
    ")\n",
    "\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "valid_iterator = DataLoader(valid_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)  # , shuffle=True)\n",
    "\n",
    "# from collections import Counter\n",
    "# Counter(prep_Xy(dataset)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 2.054 | Train Acc: 16.14%\n",
      "\t Val. Loss: 1.990 |  Val. Acc: 18.87%\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.938 | Train Acc: 24.08%\n",
      "\t Val. Loss: 1.836 |  Val. Acc: 26.62%\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.752 | Train Acc: 30.86%\n",
      "\t Val. Loss: 1.682 |  Val. Acc: 33.18%\n",
      "Epoch: 04 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.669 | Train Acc: 34.76%\n",
      "\t Val. Loss: 1.611 |  Val. Acc: 35.69%\n",
      "Epoch: 05 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.627 | Train Acc: 36.55%\n",
      "\t Val. Loss: 1.582 |  Val. Acc: 36.46%\n",
      "Epoch: 06 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 1.593 | Train Acc: 37.02%\n",
      "\t Val. Loss: 1.558 |  Val. Acc: 38.72%\n",
      "Epoch: 07 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.559 | Train Acc: 38.21%\n",
      "\t Val. Loss: 1.544 |  Val. Acc: 39.79%\n",
      "Epoch: 08 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.528 | Train Acc: 40.26%\n",
      "\t Val. Loss: 1.523 |  Val. Acc: 40.10%\n",
      "Epoch: 09 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.492 | Train Acc: 41.84%\n",
      "\t Val. Loss: 1.497 |  Val. Acc: 40.87%\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 1.444 | Train Acc: 43.89%\n",
      "\t Val. Loss: 1.466 |  Val. Acc: 41.49%\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.384 | Train Acc: 45.70%\n",
      "\t Val. Loss: 1.429 |  Val. Acc: 44.56%\n",
      "Epoch: 12 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.318 | Train Acc: 48.02%\n",
      "\t Val. Loss: 1.411 |  Val. Acc: 45.74%\n",
      "Epoch: 13 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.246 | Train Acc: 51.45%\n",
      "\t Val. Loss: 1.397 |  Val. Acc: 48.67%\n",
      "Epoch: 14 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.170 | Train Acc: 55.22%\n",
      "\t Val. Loss: 1.333 |  Val. Acc: 50.21%\n",
      "Epoch: 15 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 1.087 | Train Acc: 58.60%\n",
      "\t Val. Loss: 1.278 |  Val. Acc: 54.82%\n",
      "Epoch: 16 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 1.005 | Train Acc: 61.67%\n",
      "\t Val. Loss: 1.249 |  Val. Acc: 55.44%\n",
      "Epoch: 17 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.922 | Train Acc: 65.17%\n",
      "\t Val. Loss: 1.199 |  Val. Acc: 60.51%\n",
      "Epoch: 18 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.854 | Train Acc: 68.36%\n",
      "\t Val. Loss: 1.202 |  Val. Acc: 59.90%\n",
      "Epoch: 19 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.782 | Train Acc: 70.83%\n",
      "\t Val. Loss: 1.206 |  Val. Acc: 59.59%\n",
      "Epoch: 20 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.708 | Train Acc: 73.71%\n",
      "\t Val. Loss: 1.169 |  Val. Acc: 64.67%\n",
      "Epoch: 21 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.598 | Train Acc: 79.36%\n",
      "\t Val. Loss: 1.276 |  Val. Acc: 62.67%\n",
      "Epoch: 22 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.553 | Train Acc: 79.66%\n",
      "\t Val. Loss: 1.168 |  Val. Acc: 65.90%\n",
      "Epoch: 23 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.467 | Train Acc: 83.40%\n",
      "\t Val. Loss: 1.299 |  Val. Acc: 63.28%\n",
      "Epoch: 24 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.437 | Train Acc: 83.63%\n",
      "\t Val. Loss: 1.380 |  Val. Acc: 65.13%\n",
      "Epoch: 25 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.401 | Train Acc: 85.88%\n",
      "\t Val. Loss: 1.326 |  Val. Acc: 67.28%\n",
      "Epoch: 26 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.329 | Train Acc: 89.16%\n",
      "\t Val. Loss: 1.122 |  Val. Acc: 71.08%\n",
      "Epoch: 27 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.302 | Train Acc: 89.75%\n",
      "\t Val. Loss: 1.315 |  Val. Acc: 69.23%\n",
      "Epoch: 28 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.234 | Train Acc: 91.44%\n",
      "\t Val. Loss: 1.366 |  Val. Acc: 71.38%\n",
      "Epoch: 29 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.234 | Train Acc: 91.74%\n",
      "\t Val. Loss: 1.296 |  Val. Acc: 72.31%\n",
      "Epoch: 30 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.213 | Train Acc: 92.53%\n",
      "\t Val. Loss: 1.444 |  Val. Acc: 70.62%\n",
      "Epoch: 31 | Epoch Time: 0m 4s\n",
      "\tTrain Loss: 0.235 | Train Acc: 91.54%\n",
      "\t Val. Loss: 1.267 |  Val. Acc: 73.08%\n",
      "Epoch: 32 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.195 | Train Acc: 93.36%\n",
      "\t Val. Loss: 1.336 |  Val. Acc: 73.69%\n",
      "Epoch: 33 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.168 | Train Acc: 94.55%\n",
      "\t Val. Loss: 1.471 |  Val. Acc: 70.46%\n",
      "Epoch: 34 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.118 | Train Acc: 96.50%\n",
      "\t Val. Loss: 1.502 |  Val. Acc: 73.23%\n",
      "Epoch: 35 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.113 | Train Acc: 95.83%\n",
      "\t Val. Loss: 1.392 |  Val. Acc: 76.00%\n",
      "Epoch: 36 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.79%\n",
      "\t Val. Loss: 1.671 |  Val. Acc: 71.28%\n",
      "Epoch: 37 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.081 | Train Acc: 97.55%\n",
      "\t Val. Loss: 1.496 |  Val. Acc: 76.46%\n",
      "Epoch: 38 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.116 | Train Acc: 96.23%\n",
      "\t Val. Loss: 1.585 |  Val. Acc: 75.69%\n",
      "Epoch: 39 | Epoch Time: 0m 6s\n",
      "\tTrain Loss: 0.040 | Train Acc: 99.31%\n",
      "\t Val. Loss: 2.113 |  Val. Acc: 71.54%\n",
      "Epoch: 40 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.41%\n",
      "\t Val. Loss: 1.568 |  Val. Acc: 76.15%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12 * 2 * 54, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=8)\n",
    "\n",
    "    # comes in as 20x230. 20 = MFCCs, 230 is padded length that doesnt truncate any data\n",
    "    def forward(self, t):\n",
    "        # conv 1\n",
    "        t = self.conv1(t)  # output 6x16x226\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)  # output 6x8x113\n",
    "\n",
    "        # conv 2\n",
    "        t = self.conv2(t)  # output 12x4x109\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)  # output 12x2x54\n",
    "\n",
    "        # fc1\n",
    "        t = t.reshape(-1, 12 * 2 * 54)  # flatten to 1x1296\n",
    "        t = self.fc1(t)  # output 1x120\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # fc2\n",
    "        t = self.fc2(t)  # output 1x60\n",
    "        t = F.relu(t)\n",
    "        t = self.out(t)  # output 1x8 (8 classes)\n",
    "        # don't need softmax here since we'll use cross-entropy as activation.\n",
    "        return t\n",
    "\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    )  # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for X, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = categorical_accuracy(predictions, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in iterator:\n",
    "            predictions = model(X)\n",
    "            loss = criterion(predictions, y)\n",
    "            acc = categorical_accuracy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "# def train(model, train_loader, optimizer, epochs=10, log_interval=100, device=device):\n",
    "#     model.to(device).train()\n",
    "#     for batch_idx, (X, y) in enumerate(train_loader):\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(X)\n",
    "#         loss = F.cross_entropy(output, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % log_interval == 0:\n",
    "#             print(\n",
    "#                 \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "#                     epochs,\n",
    "#                     batch_idx * len(X),\n",
    "#                     len(train_loader.dataset),\n",
    "#                     100.0 * batch_idx / len(train_loader),\n",
    "#                     loss.item(),\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "\n",
    "# def test(model, test_loader, device=device):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in test_loader:\n",
    "#             output = model(X)\n",
    "#             test_loss += F.cross_entropy(output, y).item()  # sum up batch loss\n",
    "#             pred = output.argmax(\n",
    "#                 dim=1, keepdim=True\n",
    "#             )  # get the index of the max log-probability\n",
    "#             correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print(\n",
    "#         \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "#             test_loss,\n",
    "#             correct,\n",
    "#             len(test_loader.dataset),\n",
    "#             100.0 * correct / len(test_loader.dataset),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "model = Network().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# train(net, trainloader, optimizer)\n",
    "# test(net, testloader)\n",
    "N_EPOCHS = 40\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"classifer-model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}